{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c8feed",
   "metadata": {},
   "source": [
    "This is a filter that will act as a way to make sure that the users input is not harmful or dangerous. We can assum that here we want to filter the users response before it reaches the rest of the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c30553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/xgb_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e279321",
   "metadata": {},
   "source": [
    "Create the gaurdrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2c47cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "toxicity_classifier = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=\"unitary/toxic-bert\", \n",
    "    top_k=None # Return scores for all categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a91e6",
   "metadata": {},
   "source": [
    "Create the safety function to be implemented into the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd3a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_safety(user_text, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Returns (True, Reason) if safe.\n",
    "    Returns (False, Reason) if unsafe.\n",
    "    \"\"\"\n",
    "    results = toxicity_classifier(user_text)\n",
    "    \n",
    "    # The model returns a list of lists. We grab the first result.\n",
    "    scores = results[0]\n",
    "    \n",
    "    # Check if any category exceeds the threshold\n",
    "    for category in scores:\n",
    "        if category['score'] > threshold:\n",
    "            return False, f\"Blocked due to {category['label']} ({round(category['score'], 2)})\"\n",
    "    \n",
    "    return True, \"Safe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704c415",
   "metadata": {},
   "source": [
    "Test the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638df208",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    \"Where is my refund? I am frustrated.\",  # Negative but Safe\n",
    "    \"You are a stupid bot and I hate you.\",   # Insult\n",
    "    \"I'm going to destroy your office.\"       # Threat\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fcbd582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] Message: 'Where is my refund? I am frustrated.' -> Safe\n",
      "[BLOCK] Message: 'You are a stupid bot and I hate you.' -> Blocked due to toxic (0.99)\n",
      "[BLOCK] Message: 'I'm going to destroy your office.' -> Blocked due to toxic (0.82)\n"
     ]
    }
   ],
   "source": [
    "for msg in test_messages:\n",
    "    is_safe, reason = check_safety(msg)\n",
    "    status = \"PASS\" if is_safe else \"BLOCK\"\n",
    "    print(f\"[{status}] Message: '{msg}' -> {reason}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
