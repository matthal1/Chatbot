{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e599705c",
   "metadata": {},
   "source": [
    "This will act as the prompt generation module that will act to generate prompts in response to the users initial input. Typically here we would use an api key such as one by open AI, but I'm choosing to run this locally and will use a free model from hugging face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f24c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e57ff",
   "metadata": {},
   "source": [
    "Load a pretrained model and begin making the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e27ec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\n",
    "\n",
    "llm_pipeline = pipeline('text2text-generation', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=512, \n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ec193",
   "metadata": {},
   "source": [
    "We need to make a generation function to actually create the response to the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f75e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_query, retrieved_context, sentiment, intent):\n",
    "    # Flan-T5 prefers this specific structure:\n",
    "    prompt = f\"\"\"\n",
    "    Answer the following question based on the context provided. Be polite and helpful.\n",
    "\n",
    "    Context: {retrieved_context}\n",
    "    \n",
    "    Question: {user_query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # We call the pipeline\n",
    "    response = llm_pipeline(prompt)\n",
    "    \n",
    "    # Clean the output just in case\n",
    "    return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625c49e",
   "metadata": {},
   "source": [
    "Test this pipeline to ensure proper end user feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcfd3b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Response locally...\n",
      "\n",
      "--- FINAL CHATBOT OUTPUT ---\n",
      "5-7 business days\n"
     ]
    }
   ],
   "source": [
    "mock_user_query = \"Where is my refund? I've been waiting forever!\"\n",
    "mock_sentiment = \"negative\" \n",
    "mock_intent = \"cancellation and refund\"\n",
    "mock_retrieved_policy = \"Refund Policy: Money is returned to the original payment method within 5-7 business days.\"\n",
    "\n",
    "print(\"\\nGenerating Response locally...\")\n",
    "final_answer = generate_response(\n",
    "    mock_user_query, \n",
    "    mock_retrieved_policy, \n",
    "    mock_sentiment, \n",
    "    mock_intent\n",
    ")\n",
    "\n",
    "print(\"\\n--- FINAL CHATBOT OUTPUT ---\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66703cc1",
   "metadata": {},
   "source": [
    "Key Takeaways, this model is more straightforward which may result in user feedback that is not overly friendly. Using a model where you pay per token such as a API call from chatgpt would be better suited for an issue such as this. However for the purpose of demenstration this model does just fine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
