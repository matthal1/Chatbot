{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46051509",
   "metadata": {},
   "source": [
    "There are two goals of this section. First to generate a comparison between a friendly chat bot and a formal bot. The second objective is to validate that the response generated by the LLM exists and does not hallucinate at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf0bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import uuid\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f7e3a",
   "metadata": {},
   "source": [
    "Create the logger to compare the models outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02662454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentLogger:\n",
    "    def __init__(self, filepath=\"ab_test_logs.csv\"):\n",
    "        self.filepath = filepath\n",
    "        # Create headers if file doesn't exist\n",
    "        if not os.path.exists(filepath):\n",
    "            with open(filepath, mode='w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    \"timestamp\", \"session_id\", \"variant\", \n",
    "                    \"user_query\", \"sentiment\", \"intent\", \n",
    "                    \"retrieved_context\", \"llm_response\", \n",
    "                    \"response_time_ms\", \"validation_score\"\n",
    "                ])\n",
    "    def log(self, session_id, variant, query, sentiment, intent, context, response, latency, score):\n",
    "            with open(self.filepath, mode='a', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    datetime.now(), session_id, variant, \n",
    "                    query, sentiment, intent, \n",
    "                    context, response, \n",
    "                    round(latency * 1000, 2), score\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24192a",
   "metadata": {},
   "source": [
    "Create the quality validator of the prompt generated by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a76f06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityValidator:\n",
    "    def __init__(self):\n",
    "        # We reuse the RAG model to check if the Answer matches the Context\n",
    "        print(\"Loading Validator Model...\")\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def validate(self, llm_response, retrieved_context):\n",
    "        \"\"\"\n",
    "        Calculates a 'Hallucination Score'. \n",
    "        If the LLM answer has nothing to do with the Policy, the score will be low.\n",
    "        \"\"\"\n",
    "        # 1. Sanity Check: Is it empty or too short?\n",
    "        if not llm_response or len(llm_response) < 10:\n",
    "            return 0.0, \"Response too short\"\n",
    "\n",
    "        # 2. Semantic Consistency Check\n",
    "        # We turn both texts into numbers and measure the angle between them (Cosine Similarity)\n",
    "        embeddings = self.model.encode([llm_response, retrieved_context])\n",
    "        \n",
    "        # Calculate similarity (Range: 0 to 1)\n",
    "        score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        \n",
    "        return float(score), \"Valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb1b32a",
   "metadata": {},
   "source": [
    "Test the usability of the these two functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd95115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Validator Model...\n"
     ]
    }
   ],
   "source": [
    "logger = ExperimentLogger()\n",
    "validator = QualityValidator()\n",
    "\n",
    "def run_pipeline_step(user_query, context, sentiment, intent):\n",
    "    session_id = str(uuid.uuid4())\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- A/B TEST LOGIC ---\n",
    "    # Randomly assign user to Group A (Friendly) or Group B (Formal)\n",
    "    variant = \"A_Friendly\" if random.random() > 0.5 else \"B_Formal\"\n",
    "    \n",
    "    # (In a real app, you would change the LLM prompt here based on the variant)\n",
    "    # For this test, we simulate an LLM response\n",
    "    if variant == \"A_Friendly\":\n",
    "        mock_response = \"Sure thing! Per our policy, refunds take 5-7 days.\"\n",
    "    else:\n",
    "        mock_response = \"Refunds are processed within 5-7 business days according to policy.\"\n",
    "\n",
    "    # --- VALIDATION STEP ---\n",
    "    # Check if the answer actually matches the context\n",
    "    score, reason = validator.validate(mock_response, context)\n",
    "    \n",
    "    # --- LOGGING STEP ---\n",
    "    duration = time.time() - start_time\n",
    "    logger.log(\n",
    "        session_id=session_id,\n",
    "        variant=variant,\n",
    "        query=user_query,\n",
    "        sentiment=sentiment,\n",
    "        intent=intent,\n",
    "        context=context,\n",
    "        response=mock_response,\n",
    "        latency=duration,\n",
    "        score=score\n",
    "    )\n",
    "\n",
    "    return mock_response, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7616778",
   "metadata": {},
   "source": [
    "Test the functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aad3eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Response: Sure thing! Per our policy, refunds take 5-7 days.\n",
      "Quality Score: 0.7290 (1.0 is perfect match)\n",
      "Check 'ab_test_logs.csv' to see the data.\n"
     ]
    }
   ],
   "source": [
    "q = \"When do I get my money?\"\n",
    "ctx = \"Refund Policy: Refunds are processed in 5-7 business days.\"\n",
    "sent = \"neutral\"\n",
    "inte = \"refund\"\n",
    "\n",
    "response, quality_score = run_pipeline_step(q, ctx, sent, inte)\n",
    "\n",
    "print(f\"\\nFinal Response: {response}\")\n",
    "print(f\"Quality Score: {quality_score:.4f} (1.0 is perfect match)\")\n",
    "print(\"Check 'ab_test_logs.csv' to see the data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
